{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 13:31:22.819623: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-01 13:31:33.172676: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-01 13:31:33.172757: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-01 13:31:54.412072: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-01 13:31:54.413039: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-01 13:31:54.413096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoTokenizer, TFBertModel\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_semantics:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def load_train_and_test_data(self):\n",
    "        df_train = pd.read_csv('dataset/train.csv', header=0, sep=\";\", names=['input', 'semantics'], encoding='utf-8')\n",
    "        df_test = pd.read_csv('dataset/test.csv', header=0, sep=\",\", names=['input', 'semantics'], encoding='utf-8')\n",
    "        \n",
    "        encoded_dict = {'StreetAddress':0, 'City':1, 'State':2, 'Date':3, 'DateTime':4, 'Email':5, 'Name':6, 'Gender':7, 'Latitude':8, 'Longitude':9, 'Manufacturer':10, 'PhoneNumber':11, 'RoleTitle':12, 'ssn':13, 'Zipcode':14, 'Boolean':15}        \n",
    "        df_train['semantics'] = df_train.semantics.map(encoded_dict)\n",
    "        df_test['semantics'] = df_test.semantics.map(encoded_dict)\n",
    "\n",
    "        y_train = to_categorical(df_train.semantics)\n",
    "        y_test = to_categorical(df_test.semantics)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "        bert = TFBertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "        # Tokenise the input\n",
    "        # using tokeniser from bert-base-cased\n",
    "\n",
    "        x_train = tokenizer(\n",
    "            text=df_train.input.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length=20,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='tf',\n",
    "            return_token_type_ids='False',\n",
    "            return_attention_mask=True,\n",
    "            verbose=True)\n",
    "\n",
    "        x_test = tokenizer(\n",
    "            text=df_test.input.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length=20,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='tf',\n",
    "            return_token_type_ids='False',\n",
    "            return_attention_mask=True,\n",
    "            verbose=True)\n",
    "\n",
    "        input_ids = x_train['input_ids']\n",
    "        attention_mask = x_train['attention_mask']\n",
    "\n",
    "        max_len = 20\n",
    "        input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "        input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "        embeddings = bert(input_ids,attention_mask = input_mask)[0] \n",
    "        out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
    "        out = Dense(128, activation='relu')(out)\n",
    "        out = tf.keras.layers.Dropout(0.1)(out)\n",
    "        out = Dense(32,activation = 'relu')(out)\n",
    "        y = Dense(16,activation = 'sigmoid')(out)\n",
    "        model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "        model.layers[2].trainable = True\n",
    "\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "            learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website \n",
    "            epsilon=1e-08,\n",
    "            decay=0.01,\n",
    "            clipnorm=1.0)\n",
    "        \n",
    "        # Set loss and metrics\n",
    "        loss =CategoricalCrossentropy(from_logits = True)\n",
    "        metric = CategoricalAccuracy('balanced_accuracy'),\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(\n",
    "            optimizer = optimizer,\n",
    "            loss = loss, \n",
    "            metrics = metric)\n",
    "\n",
    "        train_history = model.fit(\n",
    "            x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n",
    "            y = y_train,\n",
    "            validation_data = (\n",
    "            {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_test\n",
    "            ),\n",
    "        epochs=1,\n",
    "            batch_size=100\n",
    "        )\n",
    "\n",
    "        model.save('BERT_model.h5')\n",
    "\n",
    "        #predicted_raw = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})\n",
    "\n",
    "        #y_predicted = np.argmax(predicted_raw, axis = 1)\n",
    "        #y_true = df_test.semantics\n",
    "\n",
    "        #from sklearn.metrics import classification_report\n",
    "        #print(classification_report(y_true, y_predicted))\n",
    "\n",
    "        return None\n",
    "\n",
    "    def bert_tagging(self):\n",
    "        \n",
    "        #self.load_train_and_test_data()\n",
    "\n",
    "        # Load the data as DataFrame\n",
    "        df = pd.read_csv(\"dataset/person_data.csv\",header=0, encoding='utf-8')\n",
    "        \n",
    "        # Fill the null values with previous values\n",
    "        df = df.bfill()\n",
    "        df = df.astype(str)\n",
    "        \n",
    "        model = tf.keras.models.load_model('BERT_model.h5', custom_objects={\"TFBertModel\": transformers.TFBertModel},compile=False)\n",
    "\n",
    "        data = df.copy()\n",
    "        row_count = len(df.axes[0])\n",
    "\n",
    "        # Define the possible classes\n",
    "        classes = ['StreetAddress', 'City', 'State', 'Date', 'DateTime', 'Email', 'Name', 'Gender', 'Latitude', 'Longitude', 'Manufacturer', 'PhoneNumber', 'RoleTitle', 'ssn', 'Zipcode', 'Boolean']\n",
    "        df_label = pd.DataFrame({'Semantic_Tags': classes})\n",
    "        df_label = df_label.set_index('Semantic_Tags')\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "        bert = TFBertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "        encoded_dict = {'StreetAddress':0, 'City':1, 'State':2, 'Date':3, 'DateTime':4, 'Email':5, 'Name':6, 'Gender':7, 'Latitude':8, 'Longitude':9, 'Manufacturer':10, 'PhoneNumber':11, 'RoleTitle':12, 'ssn':13, 'Zipcode':14, 'Boolean':15}\n",
    "\n",
    "\n",
    "        # Iterate over the rows in the DataFrame\n",
    "        for col in data.columns:\n",
    "            \n",
    "            col_list = df[col].values.tolist()\n",
    "\n",
    "            x_val = tokenizer(\n",
    "            text=col_list,\n",
    "            add_special_tokens=True,\n",
    "            max_length=20,\n",
    "            truncation=True,\n",
    "            padding='max_length', \n",
    "            return_tensors='tf',\n",
    "            return_token_type_ids = False,\n",
    "            return_attention_mask = True,\n",
    "            verbose = True)\n",
    "\n",
    "            #create new column\n",
    "            col_name = col + \"_label\"\n",
    "\n",
    "            validation = model.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})*100\n",
    "            for i, row in data.iterrows():\n",
    "                final = dict(zip(encoded_dict.keys(), validation[i]))\n",
    "                label = max(final, key=final.get)\n",
    "                data.at[i, col_name] = label\n",
    "\n",
    "            \n",
    "            for label in classes:\n",
    "                df_label.at[label, col] = ((((data[col_name]==label).sum())/row_count)*100).round(0)\n",
    "        \n",
    "        df_label.to_csv('Results_labelled.csv')\n",
    "        data.to_csv('Results_summary.csv')\n",
    "    \n",
    "        threshold = 60\n",
    "        for col in df_label.columns:\n",
    "            for j, row in df_label.iterrows():\n",
    "                if row[col] >= threshold:\n",
    "                    print(f\"{col} --- {df_label[df_label[col]>=threshold].index.tolist()} --- {row[col]}%\")\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 21s 546ms/step\n",
      "32/32 [==============================] - 23s 707ms/step\n",
      "32/32 [==============================] - 24s 737ms/step\n",
      "32/32 [==============================] - 22s 694ms/step\n",
      "32/32 [==============================] - 22s 690ms/step\n",
      "32/32 [==============================] - 23s 722ms/step\n",
      "32/32 [==============================] - 25s 770ms/step\n",
      "32/32 [==============================] - 22s 690ms/step\n",
      "32/32 [==============================] - 22s 690ms/step\n",
      "32/32 [==============================] - 22s 684ms/step\n",
      "32/32 [==============================] - 22s 696ms/step\n",
      "32/32 [==============================] - 22s 685ms/step\n",
      "32/32 [==============================] - 22s 692ms/step\n",
      "32/32 [==============================] - 22s 682ms/step\n",
      "32/32 [==============================] - 22s 690ms/step\n",
      "32/32 [==============================] - 22s 689ms/step\n",
      "32/32 [==============================] - 22s 683ms/step\n",
      "32/32 [==============================] - 22s 686ms/step\n",
      "32/32 [==============================] - 22s 698ms/step\n",
      "ID --- ['ssn'] --- 100.0%\n",
      "FirstName --- ['Name'] --- 96.0%\n",
      "LastName --- ['Name'] --- 97.0%\n",
      "Title --- ['RoleTitle'] --- 87.0%\n",
      "Gen --- ['Gender'] --- 96.0%\n",
      "Snumber --- ['ssn'] --- 97.0%\n",
      "Eaddress --- ['Email'] --- 100.0%\n",
      "Tnumber --- ['PhoneNumber'] --- 99.0%\n",
      "Mnumber --- ['PhoneNumber'] --- 100.0%\n",
      "Salary --- ['Latitude'] --- 95.0%\n",
      "Address1_Line1 --- ['StreetAddress'] --- 95.0%\n",
      "Address1_City --- ['City'] --- 92.0%\n",
      "Address1_StateOrProvince --- ['State'] --- 99.0%\n",
      "Address1_PostalCode --- ['Zipcode'] --- 84.0%\n",
      "Make --- ['Manufacturer'] --- 96.0%\n",
      "Latitude --- ['Latitude'] --- 91.0%\n",
      "Longitutde --- ['Longitude'] --- 100.0%\n",
      "CREATED_DATE --- ['Date'] --- 100.0%\n",
      "IS_CREATED --- ['Name'] --- 100.0%\n"
     ]
    }
   ],
   "source": [
    "BERT_semantics().bert_tagging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 20,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 768)         0           ['tf_bert_model_1[0][0]']        \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['global_max_pooling1d[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           4128        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 16)           528         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,413,360\n",
      "Trainable params: 108,413,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('BERT_model.h5', custom_objects={\"TFBertModel\": transformers.TFBertModel})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv('dataset/train.csv', header=0, sep=\";\", names=['input', 'semantics'], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 20        # sample size\n",
    "replace = True  # with replacement\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "df_new = df_temp.groupby('semantics', as_index=False).apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5088, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization algorithm\n",
    "#opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "            learning_rate=5                                                                                                                                                                                                 e-05, # this learning rate is for bert model , taken from huggingface website \n",
    "            epsilon=1e-08,\n",
    "            decay=0.01,\n",
    "            clipnorm=1.0)\n",
    "\n",
    "# compile the model\n",
    "old_model.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "# fit the model on old data\n",
    "old_model.fit(X_old, y_old, epochs=150, batch_size=32, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
